\chapter{ç›¸é—œæ–‡ç»è¨è«–}
\label{c:2}

%==========================================================================================
\section{Manga Vectorization and Manipulation with Procedural Simple Screentone.}
è³‡æ–™ä¾†æº:\cite{7399427}\par
å½±ç‰‡:\href{video.mp4}{å½±ç‰‡}
       
\begin{enumerate}
\item Abstractâ€”Manga are a popular artistic form around the world, and artists use simple line drawing and screentone to create all kinds of interesting productions.

æŠ½è±¡æ¼«ç•«æ˜¯å…¨ä¸–ç•Œæµè¡Œçš„è—è¡“å½¢å¼ï¼Œè—è¡“å®¶å€‘ç”¨ç°¡å–®çš„ç·šæ¢ç•«å’Œç¶²é»ä¾†å‰µä½œå„ç¨®æœ‰è¶£çš„ä½œå“ã€‚


\item Vectorization is helpful to digitally reproduce these elements for proper content and intention delivery on electronic devices. 

çŸ¢é‡åŒ–æœ‰åŠ©æ–¼æ•¸å­—è¤‡è£½é€™äº›å…ƒç´ ï¼Œä»¥ä¾¿åœ¨é›»å­è¨­å‚™ä¸Šæ­£ç¢ºå‚³é€å…§å®¹å’Œæ„åœ–ã€‚


\item Therefore, this study aims at transforming scanned Manga to a vector representation for interactive manipulation and real-time rendering with arbitrary resolution.

å› æ­¤ï¼Œæœ¬ç ”ç©¶æ—¨åœ¨å°‡æƒæçš„æ¼«ç•«è½‰æ›ç‚ºäº¤äº’å¼æ“ä½œçš„çŸ¢é‡è¡¨ç¤ºï¼Œä¸¦ä»¥ä»»æ„çš„åˆ†è¾¨ç‡é€²è¡Œå¯¦æ™‚æ¸²æŸ“ã€‚


\item Our system first decomposes the patch into rough Manga elements including possible borders and shading regions using adaptive binarization and screentone detector.

æˆ‘å€‘çš„ç³»çµ±é¦–å…ˆä½¿ç”¨è‡ªé©æ‡‰äºŒå€¼åŒ–å’Œç¶²é»æª¢æ¸¬å™¨å°‡è£œä¸åˆ†è§£æˆç²—ç³™çš„æ¼«ç•«å…ƒç´ ï¼ŒåŒ…æ‹¬å¯èƒ½çš„é‚Šç•Œå’Œé™°å½±å€åŸŸã€‚


\item We classify detected screentone into simple and complex patterns: our system extracts simple screentone properties for refining screentone borders, estimating lighting, compensating missing strokes inside screentone regions, and later resolution independently rendering with our procedural shaders.

æˆ‘å€‘å°‡æª¢æ¸¬åˆ°çš„ç¶²é»åˆ†é¡ç‚ºç°¡å–®å’Œå¾©é›œçš„æ¨¡å¼ï¼šæˆ‘å€‘çš„ç³»çµ±æå–ç°¡å–®çš„ç¶²é»å±¬æ€§ï¼Œç”¨æ–¼ç´°åŒ–ç¶²é»é‚Šç•Œï¼Œä¼°è¨ˆå…‰ç…§ï¼Œè£œå„Ÿç¶²é»å€åŸŸå…§çš„ç¼ºå¤±ç­†åŠƒï¼Œä»¥åŠå¾Œä¾†ä½¿ç”¨ç¨‹åºè‘—è‰²å™¨ç¨ç«‹æ¸²æŸ“çš„åˆ†è¾¨ç‡ã€‚


\item Our system treats the others as complex screentone areas and vectorizes them with our proposed line tracer which aims at locating boundaries of all shading regions and polishing all shading borders with the curve-based Gaussian refiner.

æˆ‘å€‘çš„ç³»çµ±å°‡å…¶ä»–è¦–ç‚ºè¤‡é›œçš„å±å¹•å€åŸŸï¼Œä¸¦ä½¿ç”¨æˆ‘å€‘æå‡ºçš„ç·šè¿½è¸ªå™¨å°å…¶é€²è¡ŒçŸ¢é‡åŒ–è™•ç†ï¼Œè©²è¿½è¸ªå™¨æ—¨åœ¨å®šä½æ‰€æœ‰é™°å½±å€åŸŸçš„é‚Šç•Œï¼Œä¸¦ä½¿ç”¨åŸºæ–¼æ›²ç·šçš„é«˜æ–¯ç´°åŒ–å™¨ä¾†æ‹‹å…‰æ‰€æœ‰é™°å½±é‚Šç•Œã€‚


\item A user can lay down simple scribbles to cluster Manga elements intuitively for the formation of semantic components, and our system vectorizes these components into shading meshes along with embedded BeÂ´zier curves as a unified foundation for consistent manipulation including pattern manipulation, deformation, and lighting addition.

ç”¨æˆ¶å¯ä»¥æ”¾ç½®ç°¡å–®çš„å¡—é´‰ï¼Œä»¥å½¢æˆèªç¾©çµ„ä»¶ï¼Œç›´è§€åœ°å°æ¼«ç•«å…ƒç´ é€²è¡Œèšé¡ï¼Œæˆ‘å€‘çš„ç³»çµ±å°‡é€™äº›çµ„ä»¶èˆ‡çŸ¢é‡åµŒå…¥çš„Be'zieræ›²ç·šä¸€èµ·çŸ¢é‡åŒ–ç‚ºè‘—è‰²ç¶²æ ¼ï¼Œä½œç‚ºåŒ…æ‹¬åœ–æ¡ˆæ“ä½œï¼Œè®Šå½¢å’Œç…§æ˜åœ¨å…§çš„ä¸€è‡´æ“ä½œçš„çµ±ä¸€åŸºç¤åŠ æˆã€‚


\item Our system can real-time and resolution independently render the shading regions with our procedural shaders and drawing borders with the curve-based shader.

æˆ‘å€‘çš„ç³»çµ±å¯ä»¥å¯¦æ™‚å’Œåˆ†è¾¨ç‡ç¨ç«‹åœ°æ¸²æŸ“é™°å½±å€åŸŸèˆ‡æˆ‘å€‘çš„ç¨‹åºè‘—è‰²å™¨å’ŒåŸºæ–¼æ›²ç·šçš„è‘—è‰²å™¨ç¹ªè£½é‚Šç•Œã€‚


\item For Manga manipulation, the proposed vector representation can be not only magnified without artifacts but also deformed easily to generate interesting results.
å°æ–¼æ¼«ç•«æ“ä½œï¼Œæ‰€æå‡ºçš„çŸ¢é‡è¡¨ç¤ºä¸åƒ…å¯ä»¥è¢«æ”¾å¤§è€Œæ²’æœ‰å½å½±ï¼Œè€Œä¸”æ˜“æ–¼è®Šå½¢ä»¥ç”¢ç”Ÿæœ‰è¶£çš„çµæœã€‚


\end{enumerate}



%==========================================================================================
\subsection{å›ç­”ä¸‹åˆ—å•é¡Œ}

\begin{enumerate}
\item  What are motivations for this work? è©²è«–æ–‡è§£æ±ºçš„å•é¡Œ  

å¯ä»¥è®“æ¼«ç•«æ”¾å¤§ä¸”æ²’æœ‰å½å½±ï¼Œè€Œä¸”èƒ½å¤ æ˜“æ–¼è®Šå½¢ã€‚


\item  What is the proposed solution? è©²è«–æ–‡è§£æ±ºçš„æ–¹æ³•

ç·šè¿½è¸ªå™¨å°å…¶é€²è¡ŒçŸ¢é‡åŒ–è™•ç†ï¼Œè©²è¿½è¸ªå™¨æ—¨åœ¨å®šä½æ‰€æœ‰é™°å½±å€åŸŸçš„é‚Šç•Œï¼Œä¸¦ä½¿ç”¨åŸºæ–¼æ›²ç·šçš„é«˜æ–¯ç´°åŒ–å™¨ä¾†æ‹‹å…‰æ‰€æœ‰é™°å½±é‚Šç•Œã€‚

å°‡é€™äº›çµ„ä»¶èˆ‡çŸ¢é‡åµŒå…¥çš„è²èŒ²æ›²ç·šä¸€èµ·çŸ¢é‡åŒ–ç‚ºè‘—è‰²ç¶²æ ¼ï¼Œä½œç‚ºåŒ…æ‹¬åœ–æ¡ˆæ“ä½œï¼Œè®Šå½¢å’Œç…§æ˜åœ¨å…§çš„ä¸€è‡´æ“ä½œçš„çµ±ä¸€åŸºç¤åŠ æˆã€‚

\item  What is the evaluation of the proposed solution?è©²è«–æ–‡ä½œè€…å¦‚ä½•è©•ä¼°å…¶çµæœ? (å•å·,é‡æ¸¬æ™‚é–“æ•¸æ“š, å¯¦é©—â€¦)

Show the result.

\item  What are the contributions? è©²è«–æ–‡çš„è²¢ç»ç‚ºå’Œ?

æœ‰æ•ˆçš„æ¼«ç•«çŸ¢é‡åŒ–å’Œå…ƒç´ åˆ†è§£çš„æ“ç¸±ç®¡é“ï¼Œèªç¾©æ§‹ä»¶çš„æ§‹å»ºå’ŒçŸ¢é‡åŒ–ï¼Œä»¥ç¨‹å¼ç°¡å–®çš„åˆ†ä½ˆè‘—è‰²ã€‚


\item  What are future directions for this research? æœªä¾†ç ”ç©¶æ–¹å‘å¯ä»¥ç‚ºä½•?

å¸Œæœ›é–‹ç™¼ä¸€å€‹å¤šå±¤æ¬¡çš„ç¶²é»æ¢æ¸¬å™¨æˆ–å…¶ä»–é¡å‹çš„çµæ§‹æ¢æ¸¬å™¨ä¾†é€²è¡Œæ›´å¼·å¤§çš„æ¢æ¸¬ã€‚

å¸Œæœ›é–‹ç™¼ä¸€å€‹è­˜åˆ¥ç¨‹åºå’Œä¸€å€‹ç¨‹åºè‘—è‰²å™¨ä¾†ç”¢ç”Ÿé€™äº›ç¾å­¸é™°å½±æ•ˆæœã€‚

å¸Œæœ›é–‹ç™¼ä¸€å€‹åŸºæ–¼Gaborçš„æ¨™è­˜ç¬¦ï¼Œç”¨æ–¼é¿å…å±¬æ€§æå–çš„è¤‡é›œçš„ç¶²é»æ¨¡å¼ã€‚

ä¸Šä¸‹æ–‡çµ„ä»¶çš„å®Œæˆä»ç„¶éœ€è¦å¤§é‡çš„æ‰‹å‹•å·¥ä½œã€‚æ›´å¤šçš„è‡ªå‹•å½¢ç‹€åˆ†è§£èˆ‡ç”¨æˆ¶äº¤äº’è¼ƒå°‘ï¼Œé¡ä¼¼æ–¼æŠ“æ–—åˆ‡å‰²æ˜¯éœ€è¦çš„ã€‚


\end{enumerate}
%==========================================================================================


\section{YOLO9000:Better, Faster, Stronger.}
è³‡æ–™ä¾†æº:\cite{RedmonF17}\par
å½±ç‰‡:\href{YOLO 9000 Better Faster Stronger.mp4}{å½±ç‰‡}
       
\begin{enumerate}
\item We introduce YOLO9000, a state-of-the-art, real-time object detection system that can detect over 9000 object categories. 

æˆ‘å€‘ä»‹ç´¹YOLO9000ï¼Œé€™æ˜¯ä¸€ç¨®å…ˆé€²çš„å¯¦æ™‚ç‰©é«”æª¢æ¸¬ç³»çµ±ï¼Œå¯ä»¥æª¢æ¸¬è¶…é9000å€‹å°åƒé¡åˆ¥ã€‚


\item First we propose various improvements to the YOLO detection method, both novel and drawn from prior work. 

é¦–å…ˆï¼Œæˆ‘å€‘æå‡ºäº†å°YOLOæª¢æ¸¬æ–¹æ³•çš„å„ç¨®æ”¹é€²ï¼Œæ—¢æœ‰æ–°ç©çš„ï¼Œä¹Ÿæœ‰å…ˆå‰çš„å·¥ä½œã€‚


\item The improved model, YOLOv2, is state-of-the-art on standard detection tasks like PASCAL VOC and COCO. 

æ”¹é€²å‹è™ŸYOLOv2æ˜¯æ¨™æº–æª¢æ¸¬ä»»å‹™ï¼ˆå¦‚PASCAL VOCå’ŒCOCOï¼‰çš„æœ€æ–°æŠ€è¡“ã€‚


\item Using a novel, multi-scale training method the same YOLOv2 model can run at varying sizes, offering an easy tradeoff between speed and accuracy. At 67 FPS, YOLOv2 gets 76.8 mAP on VOC 2007. 

ä½¿ç”¨æ–°ç©çš„å¤šå°ºåº¦è¨“ç·´æ–¹æ³•ï¼Œç›¸åŒçš„YOLOv2æ¨¡å‹å¯ä»¥ä»¥ä¸åŒçš„å°ºå¯¸é‹è¡Œï¼Œåœ¨é€Ÿåº¦å’Œæº–ç¢ºåº¦ä¹‹é–“æä¾›ç°¡å–®çš„æ¬Šè¡¡ã€‚åœ¨67 FPSï¼ŒYOLOv2åœ¨VOC 2007ä¸Šç²å¾—76.8 mAPã€‚


\item At 40 FPS, YOLOv2 gets 78.6 mAP, outperforming state-of-the-art methods like Faster RCNN with ResNet and SSD while still running significantly faster. 

åœ¨40 FPSæ™‚ï¼ŒYOLOv2ç²å¾—78.6 mAPï¼Œå„ªæ–¼æœ€å…ˆé€²çš„æ–¹æ³•ï¼Œå¦‚ä½¿ç”¨ResNetå’ŒSSDçš„Faster RCNNï¼ŒåŒæ™‚é‹è¡Œé€Ÿåº¦æ˜é¡¯æ›´å¿«ã€‚


\item Finally we propose a method to jointly train on object detection and classification. Using this method we train YOLO9000 simultaneously on the COCO detection dataset and the ImageNet classification dataset. 

æœ€å¾Œï¼Œæˆ‘å€‘æå‡ºäº†ä¸€ç¨®è¯åˆè¨“ç·´ç‰©é«”æª¢æ¸¬å’Œåˆ†é¡çš„æ–¹æ³•ã€‚ä½¿ç”¨æ­¤æ–¹æ³•ï¼Œæˆ‘å€‘åœ¨COCOæª¢æ¸¬æ•¸æ“šé›†å’ŒImageNetåˆ†é¡æ•¸æ“šé›†ä¸ŠåŒæ™‚è¨“ç·´YOLO9000ã€‚


\item Our joint training allows YOLO9000 to predict detections for object classes that donâ€™t have labelled detection data. We validate our approach on the ImageNet detection task. 

æˆ‘å€‘çš„è¯åˆåŸ¹è¨“å…è¨±YOLO9000é æ¸¬æ²’æœ‰æ¨™è¨˜æª¢æ¸¬æ•¸æ“šçš„å°åƒé¡çš„æª¢æ¸¬ã€‚æˆ‘å€‘é©—è­‰äº†ImageNetæª¢æ¸¬ä»»å‹™çš„æ–¹æ³•ã€‚


\item YOLO9000 gets 19.7 mAP on the ImageNet detection validation set despite only having detection data for 44 of the 200 classes. 

YOLO9000åœ¨ImageNetæª¢æ¸¬é©—è­‰é›†ä¸Šç²å¾—19.7 mAPï¼Œå„˜ç®¡åªæœ‰200å€‹é¡ä¸­çš„44å€‹å…·æœ‰æª¢æ¸¬æ•¸æ“šã€‚


\item On the 156 classes not in COCO, YOLO9000 gets 16.0 mAP. 

ä¸åœ¨COCOçš„156å€‹é¡åˆ¥ä¸­ï¼ŒYOLO9000ç²å¾—16.0 mAPã€‚

\item But YOLO can detect more than just 200 classes; it predicts detections for more than 9000 different object categories. And it still runs in real-time.

ä½†æ˜¯YOLOå¯ä»¥æª¢æ¸¬åˆ°è¶…é200å€‹é¡åˆ¥;å®ƒé æ¸¬è¶…é9000ç¨®ä¸åŒå°åƒé¡åˆ¥çš„æª¢æ¸¬ã€‚å®ƒä»ç„¶å¯ä»¥å¯¦æ™‚é‹è¡Œã€‚

\end{enumerate}



%==========================================================================================
\subsection{å›ç­”ä¸‹åˆ—å•é¡Œ}

\begin{enumerate}
\item  What are motivations for this work? è©²è«–æ–‡è§£æ±ºçš„å•é¡Œ  

æ˜“æ–¼å°‹æ‰¾è¦åµæ¸¬çš„ç‰©é«”ï¼Œä¸”æ•¸é‡é«˜é”9000ç¨®ï¼Œåœ¨æ™‚é–“ä¸Šèƒ½å¤ åœ¨å¯¦ç¾real-tiimeçš„åµæ¸¬ã€‚

\item  What is the proposed solution? è©²è«–æ–‡è§£æ±ºçš„æ–¹æ³•

æå‡ºäº†ä¸€ç¨®è¯åˆè¨“ç·´ç‰©é«”æª¢æ¸¬å’Œåˆ†é¡çš„æ–¹æ³•ï¼Œæ­¤æ–°ç©çš„å¤šå°ºåº¦è¨“ç·´æ–¹æ³•ï¼Œå¯ä»¥åœ¨é€Ÿåº¦å’Œæº–ç¢ºåº¦ä¹‹é–“æä¾›ç°¡å–®çš„æ¬Šè¡¡ã€‚

\item  What is the evaluation of the proposed solution?è©²è«–æ–‡ä½œè€…å¦‚ä½•è©•ä¼°å…¶çµæœ? (å•å·,é‡æ¸¬æ™‚é–“æ•¸æ“š, å¯¦é©—â€¦)

YOLO9000åœ¨ImageNetæª¢æ¸¬é©—è­‰é›†ä¸Šç²å¾—19.7 mAPè€Œä¸åœ¨COCOçš„156å€‹é¡åˆ¥ä¸­ï¼ŒYOLO9000ç²å¾—16.0 mAPã€‚

\item  What are the contributions? è©²è«–æ–‡çš„è²¢ç»ç‚ºå’Œ?

æå‡ºä¸€ç¨®æ–°ç©çš„è¨“ç·´æ–¹æ³•ï¼Œåœ¨ç‰©é«”çš„åµæ¸¬ä¸Šå¯¦ç¾real-timeã€‚

\item  What are future directions for this research? æœªä¾†ç ”ç©¶æ–¹å‘å¯ä»¥ç‚ºä½•?

å¸Œæœ›ä½¿ç”¨é¡ä¼¼çš„æŠ€è¡“é€²è¡Œweaklyçš„ç›£ç£åœ–åƒåˆ†å‰²ã€‚

å¸Œæœ›ä½¿ç”¨æ›´å¥½çš„æ¼”ç®—æ³•ä»¥æ”¹é€²åµæ¸¬çš„çµæœã€‚


\end{enumerate}

%==========================================================================================

\section{Deep Residual Learning for Image Recognition. å·ç©å½±åƒæ·±åº¦å­¸ç¿’}
è³‡æ–™ä¾†æº:{Microsoft Research.2016 IEEE\cite{DBLP:journals/corr/HeZRS15}}
å½±ç‰‡:\href{Research Talk (in Hindi) Deep Residual Learning for Image Recognition.mp4}{å½±ç‰‡}
      
\begin{enumerate}
\item Deeper neural networks are more difficult to train. 

æ›´æ·±å…¥çš„ç¥ç¶“ç¶²çµ¡æ›´é›£è¨“ç·´ã€‚

\item We present a residual learning framework to ease the training of networks that are substantially deeper than those used previously. 

æˆ‘å€‘æå‡ºäº†ä¸€å€‹æ®˜ç•™çš„å­¸ç¿’æ¡†æ¶ï¼Œä»¥ä¾¿æ–¼å°æ¯”ä»¥å‰ä½¿ç”¨çš„ç¶²çµ¡æ·±åº¦æ›´æ·±çš„ç¶²çµ¡é€²è¡ŒåŸ¹è¨“ã€‚

\item We explicitly reformulate the layers as learning residual functions with reference to the layer inputs, instead of learning unreferenced functions. 

æˆ‘å€‘æ˜ç¢ºåœ°å°‡å±¤é‡æ–°çµ„åˆç‚ºåƒè€ƒå±¤è¼¸å…¥çš„å­¸ç¿’æ®˜å·®å‡½æ•¸ï¼Œè€Œä¸æ˜¯å­¸ç¿’æœªå¼•ç”¨çš„å‡½æ•¸ã€‚

\item We provide comprehensive empirical evidence showing that these residual networks are easier to optimize, and can gain accuracy from considerably increased depth. 

æˆ‘å€‘æä¾›å…¨é¢çš„ç¶“é©—è­‰æ“šè¡¨æ˜é€™äº›æ®˜ç•™ç¶²çµ¡æ›´å®¹æ˜“å„ªåŒ–ï¼Œä¸¦ä¸”å¯ä»¥å¾é¡¯è‘—å¢åŠ çš„æ·±åº¦ç²å¾—æº–ç¢ºæ€§ã€‚

\item On the ImageNet dataset we evaluate residual nets with a depth of up to 152 layersâ€”8Ã— deeper than VGG nets [40] but still having lower complexity. 

åœ¨ImageNetæ•¸æ“šé›†ä¸Šï¼Œæˆ‘å€‘è©•ä¼°äº†æ·±åº¦é«˜é”152å±¤çš„æ®˜ç•™ç¶²çµ¡ - æ¯”VGGç¶²çµ¡æ·±8å€[40]ï¼Œä½†ä»ç„¶å…·æœ‰è¼ƒä½çš„è¤‡é›œåº¦ã€‚

\item An ensemble of these residual nets achieves 3.57ï¼… error on the ImageNet test set. This result won the 1st place on the ILSVRC 2015 classification task. 

é€™äº›æ®˜ç•™ç¶²çµ¡çš„é›†åˆåœ¨ImageNetæ¸¬è©¦é›†ä¸Šå¯¦ç¾äº†3.57ï¼…çš„èª¤å·®ã€‚è©²çµæœåœ¨ILSVRC 2015åˆ†é¡ä»»å‹™ä¸­ç²å¾—ç¬¬ä¸€åã€‚

\item We also present analysis on CIFAR-10 with 100 and 1000 layers. The depth of representations is of central importance for many visual recognition tasks. 

æˆ‘å€‘é‚„æä¾›äº†100å’Œ1000å±¤çš„CIFAR-10åˆ†æã€‚è¡¨ç¤ºçš„æ·±åº¦å°æ–¼è¨±å¤šè¦–è¦ºè­˜åˆ¥ä»»å‹™è€Œè¨€è‡³é—œé‡è¦ã€‚

\item Solely due to our extremely deep representations, we obtain a 28ï¼… relative improvement on the COCO object detection dataset. 

åƒ…åƒ…ç”±æ–¼æˆ‘å€‘æ¥µå…¶æ·±åˆ»çš„è¡¨ç¤ºï¼Œæˆ‘å€‘åœ¨COCOå°è±¡æª¢æ¸¬æ•¸æ“šé›†ä¸Šç²å¾—äº†28ï¼…çš„ç›¸å°æ”¹é€²ã€‚

\item Deep residual nets are foundations of our submissions to ILSVRC ï¼† COCO 2015 competitions1 , where we also won the 1st places on the tasks of ImageNet detection, ImageNet localization, COCO detection, and COCO segmentation.

æ·±åº¦æ®˜ç•™ç¶²æ˜¯æˆ‘å€‘å‘ILSVRCå’ŒCOCO 2015ç«¶è³½1æäº¤çš„åŸºç¤ï¼Œæˆ‘å€‘é‚„åœ¨ImageNetæª¢æ¸¬ï¼ŒImageNetå®šä½ï¼ŒCOCOæª¢æ¸¬å’ŒCOCOåˆ†å‰²ä»»å‹™ä¸­ç²å¾—äº†ç¬¬ä¸€åã€‚

\end{enumerate}

%==========================================================================================
\subsection{å›ç­”ä¸‹åˆ—å•é¡Œ}

\begin{enumerate}
\item  What are motivations for this work? è©²è«–æ–‡è§£æ±ºçš„å•é¡Œ  

è®“æ®˜ç•™ç¶²çµ¡æ›´å®¹æ˜“å„ªåŒ–ï¼Œä¸¦ä¸”å¯ä»¥å¢åŠ çš„æ·±åº¦ç¶²è·¯ç²å¾—æº–ç¢ºæ€§ã€‚


\item  What is the proposed solution? è©²è«–æ–‡è§£æ±ºçš„æ–¹æ³•

æå‡ºä¸€å€‹æ–°çš„æ·±åº¦æ®˜ç•™çš„å­¸ç¿’æ¡†æ¶ï¼ŒåŸ¹è¨“æ·±åº¦èƒ½æ¯”ä»¥å¾€çš„æ·±åº¦ç¶²è·¯æ›´æ·±ã€‚

é‡æ–°çµ„åˆåƒè€ƒå±¤çš„å‡½æ•¸ï¼Œè€Œä¸æ˜¯ä¸€ç›´æ–°å¢æ–°çš„å‡½æ•¸ã€‚


\item  What is the evaluation of the proposed solution?è©²è«–æ–‡ä½œè€…å¦‚ä½•è©•ä¼°å…¶çµæœ? (å•å·,é‡æ¸¬æ™‚é–“æ•¸æ“š, å¯¦é©—â€¦)

åœ¨ImageNetæ¸¬è©¦é›†ä¸Šå¯¦ä½œï¼Œè€Œèª¤å·®åªæœ‰3.57ï¼…


\item  What are the contributions? è©²è«–æ–‡çš„è²¢ç»ç‚ºå’Œ?

ç‚ºILSVRC(ImageNet Large Scale Visual Recognition Challenge)å’ŒCOCO ç«¶è³½çš„åŸºç¤

ImageNetå¤§è¦æ¨¡è¦–è¦ºè­˜åˆ¥æŒ‘æˆ°ï¼ˆILSVRCï¼‰è©•ä¼°å¤§è¦æ¨¡ç‰©é«”æª¢æ¸¬å’Œåœ–åƒåˆ†é¡çš„ç®—æ³•ã€‚ä¸€å€‹é«˜ç´šåˆ¥çš„å‹•æ©Ÿæ˜¯è®“ç ”ç©¶äººå“¡æ¯”è¼ƒå„ç¨®ç‰©é«”çš„æª¢æ¸¬é€²åº¦ - åˆ©ç”¨ç›¸ç•¶æ˜‚è²´çš„æ¨™ç±¤å·¥ä½œã€‚å¦ä¸€å€‹å‹•æ©Ÿæ˜¯æ¸¬é‡ç”¨æ–¼æª¢ç´¢å’Œè¨»é‡‹çš„å¤§è¦æ¨¡åœ–åƒç´¢å¼•çš„è¨ˆç®—æ©Ÿè¦–è¦ºçš„é€²å±•ã€‚

\item  What are future directions for this research? æœªä¾†ç ”ç©¶æ–¹å‘å¯ä»¥ç‚ºä½•?

ç›®å‰åªæ˜¯é€šéè¨­è¨ˆå°‡deepå±¤èˆ‡thinå±¤é€²è¡Œæ­£å‰‡åŒ–ï¼Œæœªä¾†æœƒæ”¹å–„å…¶çµæœï¼Œçµåˆæ›´å¼·çš„æ­£è¦åŒ–æ–¹å¼ã€‚

\end{enumerate}

%==========================================================================================

\section{Only Look Once, Mining Distinctive Landmarks from ConvNet for Visual Place Recognition.åªçœ‹ä¸€æ¬¡ï¼Œåœ¨ConvNetæ‰¾åˆ°ç‰¹æ®Šçš„åœ°æ¨™ï¼Œç”¨æ–¼åœ°é»è­˜åˆ¥}
è³‡æ–™ä¾†æº:{2017 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)\cite{Chen:etal:IROS2017}}
\\
å½±ç‰‡:\href{1331_VI.mp4}{å½±ç‰‡}
      
\begin{enumerate}
\item Recently, image representations derived from Convolutional Neural Networks (CNNs) have been demonstrated to achieve impressive performance on a wide variety of tasks, including place recognition.\\

æœ€è¿‘ï¼Œå·²ç¶“è­‰æ˜ä¾†è‡ªå·ç©ç¥ç¶“ç¶²çµ¡ï¼ˆCNNï¼‰çš„åœ–åƒè¡¨ç¤ºåœ¨åŒ…æ‹¬ä½ç½®è­˜åˆ¥åœ¨å…§çš„å„ç¨®ä»»å‹™ä¸Šå¯¦ç¾äº†ä»¤äººå°è±¡æ·±åˆ»çš„æ€§èƒ½ã€‚

\item In this paper, we take a step deeper into the internal structure of CNNs and propose novel CNN-based image features for place recognition by identifying salient regions and creating their regional representations directly from the convolutional layer activations.\\

åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘å€‘æ›´æ·±å…¥åœ°äº†è§£CNNçš„å…§éƒ¨çµæ§‹ï¼Œä¸¦é€šéè­˜åˆ¥é¡¯è‘—å€åŸŸä¸¦ç›´æ¥å¾å·ç©å±¤æ¿€æ´»å‰µå»ºå€åŸŸè¡¨ç¤ºï¼Œæå‡ºç”¨æ–¼ä½ç½®è­˜åˆ¥çš„æ–°ç©çš„åŸºæ–¼CNNçš„åœ–åƒç‰¹å¾µã€‚

\item A range of experiments is conducted on challenging datasets with varied conditions and viewpoints.  \\

åœ¨å…·æœ‰ä¸åŒæ¢ä»¶å’Œè§€é»çš„å…·æœ‰æŒ‘æˆ°æ€§çš„æ•¸æ“šé›†ä¸Šé€²è¡Œäº†ä¸€ç³»åˆ—å¯¦é©—ã€‚

\item These reveal superior precision-recall characteristics and robustness against both viewpoint and appearance variations for the proposed approach over the state of the art. \\

é€™äº›æ­ç¤ºäº†é‡å°ç¾æœ‰æŠ€è¡“çš„æ‰€æå‡ºçš„æ–¹æ³•çš„å„ªç•°çš„ç²¾ç¢ºå›æ†¶ç‰¹æ€§å’Œé‡å°è¦–é»å’Œå¤–è§€è®ŠåŒ–çš„ç©©å›ºæ€§ã€‚

\item By analyzing the feature encoding process of our approach, we provide insights into what makes an image presentation robust against external variations. \\

é€šéåˆ†ææˆ‘å€‘æ–¹æ³•çš„ç‰¹å¾µç·¨ç¢¼éç¨‹ï¼Œæˆ‘å€‘å¯ä»¥æ·±å…¥äº†è§£åœ–åƒå‘ˆç¾å°å¤–éƒ¨è®ŠåŒ–çš„ç©©å›ºæ€§ã€‚

\end{enumerate}

%==========================================================================================
\subsection{å›ç­”ä¸‹åˆ—å•é¡Œ}

\begin{enumerate}
\item  What are motivations for this work? è©²è«–æ–‡è§£æ±ºçš„å•é¡Œ  

å¯¦ä½œå·ç©ç¥ç¶“ç¶²è·¯ï¼Œæ”¹å–„æ¼”ç®—æ³•ï¼Œæ‡‰ç”¨åœ¨è­˜åˆ¥å€åŸŸçš„ç‰¹å¾µï¼Œä¸¦å›é¥‹ä¸€å€‹ç›¸ä¼¼çš„åœ–ç‰‡ã€‚

\item  What is the proposed solution? è©²è«–æ–‡è§£æ±ºçš„æ–¹æ³•

éƒ¨åˆ†è§£æå·ç©ç¥ç¶“ç¶²è·¯çš„é‹ä½œéç¨‹èˆ‡æ¯”è¼ƒï¼Œä»¥è¦–è¦ºåŒ–çš„æ–¹å¼å‘ˆç¾ã€‚

\item  What is the evaluation of the proposed solution?è©²è«–æ–‡ä½œè€…å¦‚ä½•è©•ä¼°å…¶çµæœ? (å•å·,é‡æ¸¬æ™‚é–“æ•¸æ“š, å¯¦é©—â€¦)

ä½¿ç”¨Caffeåœ¨TitanXçš„è¨­å‚™ä¸‹è¼¸å…¥å–®å¼µåœ–ç‰‡ï¼Œå…ˆç¶“éVGG16ç¶²çµ¡ç´„ç‚º59.4ğ‘šğ‘ ï¼Œåœ¨ç¶“éä»–å€‘åœ¨Matlabæå‡ºçš„æ–¹æ³•ç·¨è­¯ç´„349ğ‘šğ‘ ï¼Œæœ€å¾Œé€²è¡Œå…©åœ–åƒåŒ¹é…ç´„7ğ‘šğ‘ ï¼Œå¯ä»¥åœ¨è¿‘ä¹ä¸€ç§’å…§æ‰¾å‡ºä¸€å€‹ç›¸ä¼¼çš„åœ–ç‰‡ã€‚

\item  What are the contributions? è©²è«–æ–‡çš„è²¢ç»ç‚ºå’Œ?

1.ä¸€ç¨®æ–°ç©çš„åŸºæ–¼CNNçš„ç‰¹å¾µç·¨ç¢¼æ–¹æ³•ï¼Œç”¨æ–¼å‰µå»ºèƒ½å¤ æè¿°è‹¥å¹²ä¸åŒåœ–åƒå€åŸŸçš„åœ–åƒè¡¨ç¤ºï¼Œè€Œç„¡éœ€å‘CNNé¥‹é€å¤šå€‹è¼¸å…¥ã€‚

2.åŸºæ–¼å€åŸŸçš„è¦–è¦ºä½ç½®è­˜åˆ¥ç³»çµ±ï¼Œå¯åŒæ™‚è™•ç†è¦–é»å’Œæ¢ä»¶çš„è®ŠåŒ–ã€‚

\item  What are future directions for this research? æœªä¾†ç ”ç©¶æ–¹å‘å¯ä»¥ç‚ºä½•?

åœ¨æœ¬è«–æ–‡ä¸­ï¼Œæ˜¯ä½¿ç”¨CNNä¸­é è¨“ç·´éçš„å°è±¡é€²è¡Œè¾¨è­˜ï¼Œå› æ­¤æœªä¾†å¸Œæœ›æ”¹å–„CNNä»¥æé«˜æ€§èƒ½ã€‚


\end{enumerate}

%==========================================================================================

\section{Visualizing and Understanding Convolutional Networks}
è³‡æ–™ä¾†æº:{Dept. of Computer Science,New York University, USA\cite{DBLP:journals/corr/ZeilerF13}}
\\
å½±ç‰‡:\href{1331_VI.mp4}{å½±ç‰‡}
      
\begin{enumerate}

\item Large Convolutional Network models have recently demonstrated impressive classification performance on the ImageNet benchmark Krizhevsky et al. [18]. 

å¤§å‹æ²ç©ç¶²çµ¡æ¨¡å‹æœ€è¿‘åœ¨ImageNetåŸºæº–æ¸¬è©¦Krizhevskyç­‰äººä¸Šå±•ç¤ºäº†ä»¤äººå°è±¡æ·±åˆ»çš„åˆ†é¡æ€§èƒ½ã€‚

\item However there is no clear understanding of why they perform so well, or how they might be improved. 

ç„¶è€Œï¼Œæ²’æœ‰æ˜ç¢ºçš„ç†è§£ä»–å€‘ç‚ºä»€éº¼è¡¨ç¾å¦‚æ­¤ä¹‹å¥½ï¼Œæˆ–è€…å¦‚ä½•æ”¹é€²å®ƒå€‘ã€‚

\item In this paper we explore both issues. 

åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘å€‘æ¢è¨äº†é€™å…©å€‹å•é¡Œã€‚

\item We introduce a novel visualization technique that gives insight into the function of intermediate feature layers and the operation of the classifier. 

æˆ‘å€‘ä»‹ç´¹äº†ä¸€ç¨®æ–°ç©çš„å¯è¦–åŒ–æŠ€è¡“ï¼Œå¯ä»¥æ·±å…¥äº†è§£ä¸­é–“ç‰¹å¾µå±¤çš„åŠŸèƒ½å’Œåˆ†é¡å™¨çš„æ“ä½œã€‚

\item Used in a diagnostic role, these visualizations allow us to find model architectures that outperform Krizhevsky et al. 

é€™äº›å¯è¦–åŒ–ç”¨æ–¼è¨ºæ–·è§’è‰²ï¼Œä½¿æˆ‘å€‘èƒ½å¤ æ‰¾åˆ°å„ªæ–¼Krizhevskyç­‰äººçš„æ¨¡å‹æ¶æ§‹ã€‚

\item on the ImageNet classification benchmark. 

åœ¨ImageNetåˆ†é¡åŸºå‡†ä¸Šã€‚

\item We also perform an ablation study to discover the performance contribution from different model layers. 

æˆ‘å€‘é‚„é€²è¡Œæ¶ˆèç ”ç©¶ï¼Œä»¥ç™¼ç¾ä¸åŒæ¨¡å‹å±¤çš„æ€§èƒ½è²¢ç»ã€‚

\item We show our ImageNet model generalizes well to other datasets: when the softmax classifier is retrained, it convincingly beats the current state-ofthe-art results on Caltech-101 and Caltech-256 datasets.

æˆ‘å€‘å±•ç¤ºäº†æˆ‘å€‘çš„ImageNetæ¨¡å‹å¾ˆå¥½åœ°æ¦‚æ‹¬äº†å…¶ä»–æ•¸æ“šé›†ï¼šç•¶softmaxåˆ†é¡å™¨è¢«é‡æ–°è¨“ç·´æ™‚ï¼Œå®ƒä»¤äººä¿¡æœåœ°æ“Šæ•—äº†Caltech-101å’ŒCaltech-256æ•¸æ“šé›†çš„ç•¶å‰æœ€å…ˆé€²çš„çµæœã€‚

\end{enumerate}

%==========================================================================================
\subsection{å›ç­”ä¸‹åˆ—å•é¡Œ}

\begin{enumerate}
\item  What are motivations for this work? è©²è«–æ–‡è§£æ±ºçš„å•é¡Œ  

ä»‹ç´¹å·ç©ç¥ç¶“ç¶²è·¯ï¼Œä¸¦å°‡ä¸­é–“çš„layerè¦–è¦ºåŒ–å‡ºä¾†

\item  What is the proposed solution? è©²è«–æ–‡è§£æ±ºçš„æ–¹æ³•

å¯¦ä½œä¸¦æ•˜è¿°å·ç©ç¥ç¶“ç¶²è·¯çš„åŸç†ï¼Œå°‡å…¶æ­¥é©Ÿä»¥åœ–ç‰‡å±•ç¾å‡ºä¾†ã€‚

\item  What is the evaluation of the proposed solution?è©²è«–æ–‡ä½œè€…å¦‚ä½•è©•ä¼°å…¶çµæœ? (å•å·,é‡æ¸¬æ™‚é–“æ•¸æ“š, å¯¦é©—â€¦)

æ¯”Caltech-101å’ŒCaltech-256æ•¸æ“šé›†çš„åˆ†é¡çµæœæ›´ç‚ºç²¾æº–ã€‚

\item  What are the contributions? è©²è«–æ–‡çš„è²¢ç»ç‚ºå’Œ?

å…¶ç›®çš„æ˜¯è©³ç´°ä»‹ç´¹å·ç©ç¥ç¶“ç¶²è·¯çš„åŸç†ï¼Œè®“äººæ›´å®¹æ˜“äº†è§£ã€‚

\item  What are future directions for this research? æœªä¾†ç ”ç©¶æ–¹å‘å¯ä»¥ç‚ºä½•?

ç•¶æ™‚çš„å·ç©ç¥ç¶“ç¶²è·¯æ¼”ç®—éç¨‹é‚„è¼ƒç‚ºç°¡æ˜“ï¼Œå¯ä»¥æŒ‘æˆ°å…¶ä»–å·ç©ç¥ç¶“ç¶²è·¯çš„æ‡‰ç”¨ã€‚

\end{enumerate}

%==========================================================================================

\section{Visualizing Convolutional Neural Networks for Image Classification}
è³‡æ–™ä¾†æº:{Dept. of Computer Science,New York University, USA\cite{DBLP:journals/corr/abs-1804-11191}}
\\
å½±ç‰‡:\href{1331_VI.mp4}{å½±ç‰‡}
      
\begin{enumerate}

\item Deep convolutional neural networks have recently shown state of the art performance on image classiï¬cation problems. \\
æœ€è¿‘ï¼Œæ·±åº¦å·ç©ç¥ç¶“ç¶²çµ¡é¡¯ç¤ºäº†åœ–åƒåˆ†é¡å•é¡Œçš„æœ€æ–°æ€§èƒ½ã€‚

\item However, their inner workings remain a mystery to machine learning experts, particularly when compared to better studied and less complex algorithms such as SVM and Logistic Regression. 
\\ç„¶è€Œï¼Œä»–å€‘çš„å·¥ä½œä¸»è¦æ˜¯ç¥ç§˜çš„æ©Ÿæ¢°å­¸ç¿’å°ˆå®¶ï¼Œç‰¹åˆ¥æ˜¯èˆ‡æ›´å¥½çš„ç ”ç©¶å’Œä¸å¤ªè¤‡é›œçš„ç®—æ³•ï¼ˆå¦‚SVMå’ŒLogisticå›æ­¸ï¼‰ç›¸æ¯”ã€‚

\item As a result, constructing and debugging effective convolutional neural networks is time-consuming and error-prone, as it often involves a substantial amount of trial and error. \\
å› æ­¤ï¼Œæ§‹å»ºå’Œèª¿è©¦æœ‰æ•ˆçš„æ²ç©ç¥ç¶“ç¶²çµ¡æ—¢è²»æ™‚åˆå®¹æ˜“å‡ºéŒ¯ï¼Œå› ç‚ºå®ƒç¶“å¸¸æ¶‰åŠå¤§é‡çš„åè¤‡è©¦é©—ã€‚

\item We introduce deepViz, a system designed to allow experts to understand their models and diagnose issues with the model structure, enabling more rapid iteration during the model construction process and faster convergence to a suitable model for the task at hand.\\
æˆ‘å€‘ä»‹ç´¹äº†deepVizï¼Œé€™æ˜¯ä¸€å€‹ç³»çµ±ï¼Œæ—¨åœ¨è®“å°ˆå®¶äº†è§£ä»–å€‘çš„æ¨¡å‹ä¸¦è¨ºæ–·æ¨¡å‹çµæ§‹çš„å•é¡Œï¼Œåœ¨æ¨¡å‹æ§‹å»ºéç¨‹ä¸­å¯¦ç¾æ›´å¿«é€Ÿçš„è¿­ä»£ï¼Œä¸¦ä¸”æ›´å¿«æ”¶æ–‚åˆ°æ‰‹é ­ä»»å‹™çš„åˆé©æ¨¡å‹ã€‚

\end{enumerate}

%==========================================================================================
\subsection{å›ç­”ä¸‹åˆ—å•é¡Œ}

\begin{enumerate}
\item  What are motivations for this work? è©²è«–æ–‡è§£æ±ºçš„å•é¡Œ  

è§£æå·ç©ç¥ç¶“ç¶²è·¯çš„å…§å®¹ï¼Œåœ¨ä»¥è¦–è¦ºåŒ–å‘ˆç¾

\item  What is the proposed solution? è©²è«–æ–‡è§£æ±ºçš„æ–¹æ³•

è©²è«–æ–‡é–‹ç™¼äº†Deep Viz å¯ä»¥å°‡å·ç©ç¥ç¶“ç¶²è·¯è§£æä¸¦å‘ˆç¾

\item  What is the evaluation of the proposed solution?è©²è«–æ–‡ä½œè€…å¦‚ä½•è©•ä¼°å…¶çµæœ? (å•å·,é‡æ¸¬æ™‚é–“æ•¸æ“š, å¯¦é©—â€¦)

èƒ½å¤ è®“å°ˆå®¶å¿«é€Ÿçš„äº†è§£å·ç©ç¥ç¶“ç¶²è·¯çš„æ¶æ§‹ï¼Œä»¥ä¾¿å¿«é€Ÿåˆ†æ

\item  What are the contributions? è©²è«–æ–‡çš„è²¢ç»ç‚ºå’Œ?

æå‡ºäº†ä¸€å€‹å¯è¦–åŒ–å·¥å…·deepVizï¼Œæ—¨åœ¨å¹«åŠ©å°ˆå®¶ç†è§£å’Œè¨ºæ–·å·ç©ç¥ç¶“ç¶²çµ¡è¦–è¦ºåˆ†é¡å•é¡Œã€‚

\item  What are future directions for this research? æœªä¾†ç ”ç©¶æ–¹å‘å¯ä»¥ç‚ºä½•?

æœªä¾†å¯æ”¹å–„å…¶å¯è¦–åŒ–çš„æŠ€è¡“ï¼Œä»¥æ›´å¥½çš„æ–¹å¼å‘ˆç¾

\end{enumerate}

%==========================================================================================

\section{Visualization of Neural Network Predictions for Weather Forecasting}
è³‡æ–™ä¾†æº:{COMPUTER GRAPHICS forumVolume 00 (2018), number 0 pp. 1â€“12 \cite{Roesch2017VisualizationON}}
\\
å½±ç‰‡:\href{Visualization of Neural Network Predictions for Weather Forecasting (VMV 2017).mp4}{å½±ç‰‡}
      
\begin{enumerate}

\item Recurrent neural networks are prime candidates for learning evolutions in multi-dimensional time series data. \\
éè¿´ç¥ç¶“ç¶²çµ¡æ˜¯ç”¨æ–¼å­¸ç¿’å¤šç¶­æ™‚é–“åºåˆ—æ•¸æ“šä¸­çš„æ¼”åŒ–çš„ä¸»è¦è€…ã€‚\\

\item The performance of such a network is judged by the loss function, which is aggregated into a scalar value that decreases during training. \\
é€™ç¨®ç¶²çµ¡çš„æ€§èƒ½ç”±æå¤±å‡½æ•¸åˆ¤æ–·ï¼Œè©²å‡½æ•¸è¢«èšåˆæˆåœ¨è¨“ç·´æœŸé–“æ¸›å°çš„æ¨™é‡å€¼ã€‚\\

\item Observing only this number hides the variation that occurs within the typically large training and testing data sets. \\
åƒ…è§€å¯Ÿæ­¤æ•¸å­—æœƒéš±è—é€šå¸¸è¼ƒå¤§çš„è¨“ç·´å’Œæ¸¬è©¦æ•¸æ“šé›†ä¸­ç™¼ç”Ÿçš„è®ŠåŒ–ã€‚\\

\item Understanding these variations is of highest importance to adjust network hyper-parameters, such as the number of neurons, number of layers or to adjust the training set to include more representative examples. \\
ç†è§£é€™äº›è®ŠåŒ–å°æ–¼èª¿æ•´ç¶²çµ¡è¶…åƒæ•¸æ˜¯æœ€é‡è¦çš„ï¼Œä¾‹å¦‚ç¥ç¶“å…ƒçš„æ•¸é‡ï¼Œå±¤æ•¸æˆ–èª¿æ•´è¨“ç·´é›†ä»¥åŒ…æ‹¬æ›´å…·ä»£è¡¨æ€§çš„ç¤ºä¾‹ã€‚\\

\item In this paper, we design a comprehensive and interactive system that allows users to study the output of recurrent neural networks on both the complete training data and testing data. \\
åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘å€‘è¨­è¨ˆäº†ä¸€å€‹å…¨é¢çš„äº¤äº’å¼ç³»çµ±ï¼Œå…è¨±ç”¨æˆ¶åœ¨å®Œæ•´çš„è¨“ç·´æ•¸æ“šå’Œæ¸¬è©¦æ•¸æ“šä¸Šç ”ç©¶éæ­¸ç¥ç¶“ç¶²çµ¡çš„è¼¸å‡ºã€‚\\

\item We follow a coarse-to-fine strategy, providing overviews of annual, monthly and daily patterns in the time series and directly support a comparison of different hyper-parameter settings. \\
æˆ‘å€‘éµå¾ªç²—ç•¥åˆ°ç²¾ç´°çš„ç­–ç•¥ï¼Œæä¾›æ™‚é–“åºåˆ—ä¸­å¹´åº¦ï¼Œæœˆåº¦å’Œæ—¥å¸¸æ¨¡å¼çš„æ¦‚è¿°ï¼Œä¸¦ç›´æ¥æ”¯æŒä¸åŒè¶…åƒæ•¸è¨­ç½®çš„æ¯”è¼ƒã€‚\\

\item We applied our method to a recurrent convolutional neural network that was trained and tested on 25 years of climate data to forecast meteorological attributes, such as temperature, pressure and wind velocity. \\
æˆ‘å€‘å°‡æˆ‘å€‘çš„æ–¹æ³•æ‡‰ç”¨æ–¼å¾ªç’°å·ç©ç¥ç¶“ç¶²çµ¡ï¼Œè©²ç¶²çµ¡ç¶“é25å¹´çš„æ°£å€™æ•¸æ“šè¨“ç·´å’Œæ¸¬è©¦ï¼Œä»¥é æ¸¬æ°£è±¡å±¬æ€§ï¼Œå¦‚æº«åº¦ï¼Œå£“åŠ›å’Œé¢¨é€Ÿã€‚\\

\item We further visualize the quality of the forecasting models, when applied to various locations on the Earth and we examine the combination of several forecasting models.\\
æˆ‘å€‘é€²ä¸€æ­¥å¯è¦–åŒ–é æ¸¬æ¨¡å‹çš„è³ªé‡ï¼Œç•¶æ‡‰ç”¨æ–¼åœ°çƒä¸Šçš„å„å€‹ä½ç½®æ™‚ï¼Œæˆ‘å€‘æª¢æŸ¥äº†å¹¾ç¨®é æ¸¬æ¨¡å‹çš„çµ„åˆã€‚

\end{enumerate}

%==========================================================================================
\subsection{å›ç­”ä¸‹åˆ—å•é¡Œ}

\begin{enumerate}
\item  What are motivations for this work? è©²è«–æ–‡è§£æ±ºçš„å•é¡Œ  

RNNçš„å¸¸ç”¨æ–¹æ³•æ˜¯ä½¿ç”¨ä¸€ç³»åˆ—å…ˆå‰æ™‚é–“æ­¥é©Ÿä¾†é æ¸¬ä¸‹ä¸€æ­¥é©Ÿã€‚æœƒå°è‡´ç„¡æ³•ä¸€æ¬¡é¡¯ç¤ºçš„éå¸¸å¤§çš„æ•¸æ“šé›†ã€‚

\item  What is the proposed solution? è©²è«–æ–‡è§£æ±ºçš„æ–¹æ³•

è¨­è¨ˆäº†ä¸€å€‹å…¨é¢çš„äº¤äº’å¼ç³»çµ±ï¼Œå…è¨±ç”¨æˆ¶åœ¨å®Œæ•´çš„è¨“ç·´æ•¸æ“šå’Œæ¸¬è©¦æ•¸æ“šä¸Šç ”ç©¶éæ­¸ç¥ç¶“ç¶²çµ¡çš„è¼¸å‡ºã€‚

\item  What is the evaluation of the proposed solution?è©²è«–æ–‡ä½œè€…å¦‚ä½•è©•ä¼°å…¶çµæœ? (å•å·,é‡æ¸¬æ™‚é–“æ•¸æ“š, å¯¦é©—â€¦)

ä½¿ç”¨1990å¹´è‡³1999å¹´çš„æ•¸æ“šä¾†è¨“ç·´1990å¹´è‡³2016å¹´çš„æ¨¡å‹å’Œæ•¸æ“šã€‚è¨“ç·´æ™‚é–“ç‚º6å°æ™‚ã€‚è©²åˆ†è¾¨ç‡åœ¨æˆ‘å€‘çš„åˆå§‹æ¸¬è©¦ä¸­è¡¨ç¾æœ€ä½³ã€‚

\item  What are the contributions? è©²è«–æ–‡çš„è²¢ç»ç‚ºå’Œ?

è©•ä¼°èª¤å·®åœ¨2016å¹´çš„æº«åº¦é æ¸¬ä¸­å–å¹³å‡å€¼ï¼Œä¸¦åœ¨æœ€å¤§èª¤å·®ç‚ºè¯æ°10åº¦ã€‚

\item  What are future directions for this research? æœªä¾†ç ”ç©¶æ–¹å‘å¯ä»¥ç‚ºä½•?

æ”¹å–„ç¾åœ¨çš„ç¥ç¶“ç¶²è·¯ç³»çµ±ï¼ŒåŠ ä¸Šæ®˜å·®ç¥ç¶“ç¶²è·¯ï¼Œä»¥ç”¨ä¾†è¨“ç·´åŠé æ¸¬å…¶ä»–å±¬æ€§ã€‚

\end{enumerate}

%==========================================================================================

\section{Deep learning for computational biology}
è³‡æ–™ä¾†æº:{COMPUTER GRAPHICS forumVolume 00 (2018), number 0 pp. 1â€“12 \cite{Roesch2017VisualizationON}}
\\
å½±ç‰‡:\href{Visualization of Neural Network Predictions for Weather Forecasting (VMV 2017).mp4}{å½±ç‰‡}
      
\begin{enumerate}

\item Technological advances in genomics and imaging have led to an explosion of molecular and cellular profiling data from large numbers of samples.  \\
åŸºå› å­¸å’Œè¦–è¦ºåŒ–æŠ€è¡“çš„é€²æ­¥å°è‡´å¤§é‡åˆ†å­å’Œç´°èƒæ¨£æœ¬çš„åˆ†ææ•¸æ“šä»¥çˆ†ç‚¸å¼çš„å¢é•·ã€‚\\

\item This rapid increase in biological data dimension and acquisition rate is challenging conventional analysis strategies.  \\
ç”Ÿç‰©æ•¸æ“šç¶­åº¦å’Œæ¡é›†é€Ÿç‡çš„é€™ç¨®å¿«é€Ÿå¢é•·æ­£åœ¨æŒ‘æˆ°å‚³çµ±çš„åˆ†æç­–ç•¥ã€‚\\

\item Modern machine learning methods, such as deep learning, promise to leverage very large data sets for finding hidden structure within them, and for making accurate predictions.  \\
ç¾ä»£æ©Ÿå™¨å­¸ç¿’æ–¹æ³•ï¼Œä¾‹å¦‚æ·±åº¦å­¸ç¿’ï¼Œæœ‰æœ›åˆ©ç”¨å¤§æ•¸æ“šä¾†å°‹æ‰¾å…¶ä¸­çš„éš±è—çµæ§‹ï¼Œä¸¦é€²è¡Œæº–ç¢ºçš„é æ¸¬ã€‚\\

\item In this review, we discuss applications of this new breed of analysis approaches in regulatory genomics and cellular imaging.  \\
åœ¨æœ¬è«–æ–‡ä¸­ï¼Œæˆ‘å€‘è¨è«–äº†é€™ç¨®æ–°å‹åˆ†ææ–¹æ³•åœ¨ç›£ç®¡åŸºå› å­¸å’Œç´°èƒæˆåƒä¸­çš„æ‡‰ç”¨ã€‚\\

\item We provide background of what deep learning is, and the settings in which it can be successfully applied to derive biological insights.  \\
æˆ‘å€‘æä¾›æ·±åº¦å­¸ç¿’çš„èƒŒæ™¯ï¼Œä»¥åŠå¯ä»¥æˆåŠŸæ‡‰ç”¨ä»¥ç²å¾—ç”Ÿç‰©å­¸è¦‹è§£çš„è¨­ç½®ã€‚\\

\item In addition to presenting specific applications and providing tips for practical use, we also highlight possible pitfalls and limitations to guide computational biologists when and how to make the most use of this new technology. \\
é™¤äº†æä¾›å…·é«”æ‡‰ç”¨ä¸¦æä¾›å¯¦éš›ä½¿ç”¨æŠ€å·§å¤–ï¼Œæˆ‘å€‘é‚„å¼·èª¿äº†å¯èƒ½å­˜åœ¨çš„ç¼ºé™·å’Œå±€é™æ€§ï¼Œä»¥æŒ‡å°è¨ˆç®—ç”Ÿç‰©å­¸å®¶ä½•æ™‚ä»¥åŠå¦‚ä½•å……åˆ†åˆ©ç”¨é€™é …æ–°æŠ€è¡“ã€‚\\


\end{enumerate}

%==========================================================================================
\subsection{å›ç­”ä¸‹åˆ—å•é¡Œ}

\begin{enumerate}
\item  What are motivations for this work? è©²è«–æ–‡è§£æ±ºçš„å•é¡Œ  

åœ¨åŸºå› å­¸åŠç´°èƒæˆåƒæˆç†Ÿçš„ç¾ä»Šï¼Œèƒ½å¤ å¿«é€Ÿè¾¨è­˜èˆ‡é æ¸¬æ˜¯å…¶ä¸­æœ€é‡è¦çš„äº‹é …ã€‚

\item  What is the proposed solution? è©²è«–æ–‡è§£æ±ºçš„æ–¹æ³•

åˆ©ç”¨æ©Ÿå™¨å­¸ç¿’ã€ç›£ç£å¼å­¸ç¿’ä»¥åŠå·ç©ç¥ç¶“ç¶²è·¯ä½œç‚ºç‰©ä»¶åµæ¸¬èˆ‡åˆ†é¡ã€‚

\item  What is the evaluation of the proposed solution?è©²è«–æ–‡ä½œè€…å¦‚ä½•è©•ä¼°å…¶çµæœ? (å•å·,é‡æ¸¬æ™‚é–“æ•¸æ“š, å¯¦é©—â€¦)

åœ¨æ›´æ·±å±¤æ•¸éšå±¤ä¸­ï¼Œæ›´èƒ½æ‰¾åˆ°åµæ¸¬ç‰©é«”çš„ç‰¹å¾µï¼Œæœƒæ›´å®¹æ˜“é€²è¡Œåˆ†é¡ã€‚

\item  What are the contributions? è©²è«–æ–‡çš„è²¢ç»ç‚ºå’Œ?

è®“è¨ˆç®—ç”Ÿç‰©å­¸å®¶å°‹æ‰¾åŸºå› å­¸ä¸­çš„éš±è—çµæ§‹ï¼Œä¸¦é€²è¡Œæº–ç¢ºçš„é æ¸¬ã€‚

\item  What are future directions for this research? æœªä¾†ç ”ç©¶æ–¹å‘å¯ä»¥ç‚ºä½•?

æ”¹å–„åƒæ•¸èˆ‡å„ªåŒ–ã€‚ä½¿è»Ÿé«”èƒ½å¤ ä½¿ç”¨æ·±åº¦å­¸ç¿’åœ¨æ›´å¤šçš„ç”Ÿç‰©å­¸å•é¡Œä¸Šã€‚

\end{enumerate}